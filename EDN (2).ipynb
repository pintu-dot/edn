{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81e87209-e9c2-4f8a-8497-1d87ad738107",
   "metadata": {},
   "outputs": [],
   "source": [
    "lam=1e-4    # Regularisation parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f39b3d17-077d-4f05-a3e2-f2fcdb89b3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import networkx as nx\n",
    "from sklearn.metrics import f1_score\n",
    "import torch_geometric\n",
    "import random\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4637c5ac-4b03-4aec-97f9-44db7818d0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(model,data,lmbda):       # training and testing GCN models\n",
    "    data.to(device)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def train():\n",
    "        model.train()\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        reg = sum(p.pow(2.0).sum()\n",
    "                  for p in model.parameters())\n",
    "        out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])+lmbda*reg  # Compute the loss solely based on the training nodes.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        return loss\n",
    "\n",
    "    def test():\n",
    "        model.eval()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "        test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "        mac_f1=f1_score(data.y[data.test_mask].cpu(), pred[data.test_mask].cpu(), average='macro');\n",
    "        return test_acc,mac_f1\n",
    "\n",
    "\n",
    "    for epoch in range(1, 801):\n",
    "        loss = train()\n",
    "\n",
    "    test_acc,mac_f1 = test()\n",
    "    return test_acc,mac_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff234892-88d3-4052-adfc-585587bf3218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_val(model,data,lmbda):\n",
    "    data.to(device)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def train():\n",
    "        model.train()\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        reg = sum(p.pow(2.0).sum()\n",
    "                  for p in model.parameters())\n",
    "        out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])+lmbda*reg  # Compute the loss solely based on the training nodes.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        return loss\n",
    "\n",
    "    def val():\n",
    "        model.eval()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        val1=((True^data.train_mask)*(True^data.test_mask))\n",
    "        val_correct = pred[val1] == data.y[val1]  # Check against ground-truth labels.\n",
    "        val_acc = int(val_correct.sum()) / int(val1.sum())  # Derive ratio of correct predictions.\n",
    "        return val_acc\n",
    "    \n",
    "    def test():\n",
    "      model.eval()\n",
    "      out = model(data.x, data.edge_index)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "      mac_f1=f1_score(data.y[data.test_mask].cpu(), pred[data.test_mask].cpu(), average='macro');\n",
    "      return test_acc,mac_f1\n",
    "\n",
    "    for epoch in range(1, 801):\n",
    "        loss = train()\n",
    "\n",
    "    val_acc = val()\n",
    "    test_acc,mac_f1=test()\n",
    "\n",
    "    #print(f'Test Accuracy: {test_acc:.4f}')\n",
    "    return val_acc,test_acc,mac_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6a9e349-12c6-4671-9a98-52449459464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SGCN(torch.nn.Module):\n",
    "    def __init__(self,hidden_channels=16,seed=13):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.conv1 = GCNConv(dataset.num_features,10)\n",
    "        self.conv2=GCNConv(10,dataset.num_classes)\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x=F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5f39f90-456e-4450-a040-01f041e97e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_noisy(data,noise_level):   # Function to add SLN noise to data\n",
    "    data1=data.clone()\n",
    "\n",
    "    rand_num=torch.rand(len(data.y))\n",
    "    for k in range(no_of_classes):\n",
    "        for j in range(no_of_classes-1):\n",
    "            pos_to_flip=(((rand_num<(j+1)*noise_level/(no_of_classes-1)).to(device))*(((j)*noise_level/(no_of_classes-1)<=rand_num).to(device))*data.train_mask)*(data.y==k)\n",
    "            data1.y[pos_to_flip]=(data1.y[pos_to_flip]+(j+1))%no_of_classes\n",
    "    return(data1)\n",
    "\n",
    "def create_noisy_ccn(data,noise_level):\n",
    "    data1=data.clone()\n",
    "    rand_num=torch.rand(len(data.y))\n",
    "    pos_to_flip=(rand_num<=noise_level).to(device)*data1.train_mask\n",
    "    data1.y[pos_to_flip]=(data.y[pos_to_flip]+1)%no_of_classes\n",
    "    del data\n",
    "    torch.cuda.empty_cache()\n",
    "    return(data1)\n",
    "\n",
    "def create_noisy_seq_pw(data,noise_level):   # add SLN noise to data\n",
    "    data1=data.clone()\n",
    "    data.to(device)\n",
    "    data1.to(device)\n",
    "    for i in range(1,no_of_classes):\n",
    "        noise_level[:,i]=noise_level[:,i]+noise_level[:,i-1]\n",
    "    rand_num=torch.rand(len(data.y))\n",
    "    for k in range(no_of_classes):\n",
    "        for j in range(no_of_classes-1):\n",
    "            pos_to_flip=(((rand_num>=noise_level[:,j]).to(device))*((noise_level[:,j+1]>rand_num).to(device))*data.train_mask)*(data.y==k)\n",
    "            #print((rand_num>=noise_level[:,j+1]).sum())\n",
    "            data1.y[pos_to_flip]=(data1.y[pos_to_flip]+(j+1))%no_of_classes\n",
    "    return(data1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9df33656-29b0-4816-b703-9c4b36e5cf8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Planetoid(root='data/py311/Planetoid', name='Citeseer', split=\"random\", num_train_per_class=250,num_val=50,num_test=1000,transform=NormalizeFeatures())\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "datap=data.clone()\n",
    "no_of_classes=int((data.y.max()+1).detach())\n",
    "datap.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cbe97e7-25e2-4a06-ac4c-eb53a83bc423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def majvote(n,p):\n",
    "    val=np.float128(0);\n",
    "    p=np.float128(p)\n",
    "    for i in range(math.ceil(n/2),n+1):\n",
    "        val=val+np.float128(math.comb(n,i))*(p**i)*((1-p)**(n-i))\n",
    "    return np.float32(val)\n",
    "\n",
    "def q(p,max_degree):\n",
    "    q1=[]\n",
    "    for n in range(1,int(max_degree)+1):\n",
    "        q1.append(majvote(n,p))\n",
    "    return q1\n",
    "def veto_power(n,p):\n",
    "    return(1-((1-p)**n))\n",
    "\n",
    "def r(p,max_degree):\n",
    "    r1=[]\n",
    "    for n in range(1,int(max_degree)+1):\n",
    "        r1.append(veto_power(n,p))\n",
    "    return r1\n",
    "def sequential_mod(n,p):\n",
    "    k=no_of_classes\n",
    "    return (((k-1)/k)*(1-(1-(k*p)/(k-1))**(n)))\n",
    "\n",
    "def s_mod(p,max_degree):\n",
    "    r1=[]\n",
    "    for n in range(1,int(max_degree)+1):\n",
    "        r1.append(sequential_mod(n,p))\n",
    "    return r1\n",
    "\n",
    "def seq_pw(n,p):\n",
    "    k=no_of_classes\n",
    "    a=[]\n",
    "    for j in range(k):\n",
    "        temp=np.float128(0*p);\n",
    "        p=np.float128(p)\n",
    "        for m in range(j,n+1,k):\n",
    "            temp=temp+np.float128(math.comb(n,m))*(p**m)*((1-p)**(n-m))\n",
    "        a.append(temp)\n",
    "    return(a)\n",
    "\n",
    "def s_pw(p,max_degree):\n",
    "    r1=[]\n",
    "    for n in range(1,int(max_degree)+1):\n",
    "        r1.append(seq_pw(n,p))\n",
    "    return r1\n",
    "def s_pw_exp(p,max_degree):\n",
    "    r1=[]\n",
    "    for n in tqdm(range(1,int(max_degree)+1)):\n",
    "        r1.append((np.array(seq_pw(n,p))[1:]).sum(axis=0))\n",
    "        #r1.append((np.array(seq_pw(n,p))[1:]).sum())\n",
    "    return r1\n",
    "    \n",
    "    \n",
    "def s(p,max_degree):  # for sequential flipping\n",
    "    s1=[p]\n",
    "    temp=p;\n",
    "    for n in range(2,int(max_degree)+1):\n",
    "        temp=p+temp*(1-2*p)\n",
    "        s1.append(temp)\n",
    "    return s1\n",
    "\n",
    "def rho_calculate(data,type='majority_vote',):\n",
    "    g_nx=torch_geometric.utils.convert.to_networkx(data,to_undirected='True',remove_self_loops='True')\n",
    "    max_degree=np.max(np.array(g_nx.degree)[:,1])/2\n",
    "    unique, counts = np.unique(np.array(g_nx.degree)[:,1], return_counts=True)\n",
    "    unique=unique/2\n",
    "    no_nodes=counts.sum()\n",
    "    x = np.linspace(0,1,5000)\n",
    "    y = expected_noise(x,unique,counts,max_degree,no_nodes,type)\n",
    "    noise_lev=[0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5]\n",
    "    x_intersect=np.zeros(len(noise_lev))\n",
    "    for j,i in enumerate(noise_lev):\n",
    "        x_intersect[j]=(x[np.argmin(np.abs(y - i))])\n",
    "    return x_intersect\n",
    "def expected_noise(p,unique,counts,max_degree,no_nodes,type='majority_vote'):\n",
    "    if type=='majority_vote':\n",
    "        qn=q(p,max_degree)\n",
    "    elif type=='veto_power':\n",
    "        qn=r(p,max_degree)\n",
    "    elif type=='sequential':\n",
    "        qn=s_mod(p,max_degree)\n",
    "    elif type=='seq_pw':\n",
    "        qn=s_pw_exp(p,max_degree)\n",
    "        #print(qn)\n",
    "    deg_dist=np.zeros(int(max_degree))\n",
    "    for i,j in enumerate(unique):\n",
    "        deg_dist[int(j)-1]=counts[i]/no_nodes\n",
    "    exp_noise=np.dot(deg_dist,np.array(qn))\n",
    "    return exp_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6c84f71-149b-4e88-b8d2-20d6eed2780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "g_nx=torch_geometric.utils.convert.to_networkx(data,to_undirected='True',remove_self_loops='True')\n",
    "max_degree=np.max(np.array(g_nx.degree)[:,1])/2\n",
    "degree_list=(np.array(g_nx.degree)[:,1])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa5eeb99-2af6-4b22-bba2-e7bbced98690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31fedd622e0c43ba84a1cfce496a1cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#estimating rho_values\n",
    "rho_mv=rho_calculate(data,'majority_vote')\n",
    "rho_veto=rho_calculate(data, 'veto_power')\n",
    "rho_seq=rho_calculate(data, 'sequential')\n",
    "rho_seq_pw=rho_calculate(data, 'seq_pw')\n",
    "\n",
    "                       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aa9e5ce-de68-421d-8b6a-2635a3bbc0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_nx=torch_geometric.utils.convert.to_networkx(data,to_undirected='True',remove_self_loops='True')\n",
    "max_degree=np.max(np.array(g_nx.degree)[:,1])/2\n",
    "unique, counts = np.unique(np.array(g_nx.degree)[:,1], return_counts=True)\n",
    "unique=unique/2\n",
    "no_nodes=counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d9da14-9095-4e36-9045-de0d53a924b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "978102bb-4a14-476c-809d-06c7e150c286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise level = 0.4\n",
      "sln noise: 0.5894999999999999 +- 0.01150000000000001\n",
      "majority vote noise: 0.5894999999999999 +- 0.0005000000000000004\n",
      "veto noise: 0.5155000000000001 +- 0.008500000000000008\n",
      "sequential  noise: 0.552 +- 0.014999999999999958\n"
     ]
    }
   ],
   "source": [
    "noise_levels=[0.4]  #One can add more noise labels as a list \n",
    "iter=2    # iter is number of times you want the experiment to be repeated,  In paper iter is 10\n",
    "for nl in noise_levels:\n",
    "    print(\"Noise level =\",nl)\n",
    "    pos=int(nl/0.05)-1\n",
    "    test_acc_sln=np.zeros(iter)\n",
    "\n",
    "    qn=q(rho_mv[pos],max_degree)\n",
    "    nl_majvote=torch.zeros(len(data.y))\n",
    "    for i in range(len(degree_list)):\n",
    "        nl_majvote[i]=float(qn[int(degree_list[i])-1])\n",
    "    nl_majvote.to(device)\n",
    "    test_acc_mv=np.zeros(iter)\n",
    "\n",
    "    qn=r(rho_veto[pos],max_degree)\n",
    "    nl_veto=torch.zeros(len(data.y))\n",
    "    for i in range(len(degree_list)):\n",
    "        nl_veto[i]=qn[int(degree_list[i])-1]\n",
    "    nl_veto.to(device)\n",
    "    test_acc_veto=np.zeros(iter)\n",
    "\n",
    "    qn=s_mod(rho_seq[pos],max_degree)\n",
    "    nl_seq=torch.zeros(len(data.y))\n",
    "    for i in range(len(degree_list)):\n",
    "        nl_seq[i]=qn[int(degree_list[i])-1]\n",
    "    nl_seq.to(device)\n",
    "    test_acc_seq=np.zeros(iter)\n",
    "    \n",
    "    for i in range(iter):\n",
    "        torch.manual_seed(i+2)\n",
    "        dataset = Planetoid(root='data/py311/Planetoid', name='Citeseer', split=\"random\", num_train_per_class=250,num_val=50,num_test=1000,transform=NormalizeFeatures())\n",
    "        datap = (dataset[0]).to(device)\n",
    "        data_bn=create_noisy(datap,nl)  \n",
    "        model_sgcn=SGCN(hidden_channels=16,seed=i+2)     # calculating accuracy on clean data set for comaprision\n",
    "        test_acc_sln[i],mac=train_and_test(model_sgcn,data_bn,lam)\n",
    "        del model_sgcn\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        torch.manual_seed(i+2)\n",
    "        data_bn=create_noisy(datap,nl_majvote)  \n",
    "        model_sgcn=SGCN(hidden_channels=16,seed=i+2)     # calculating accuracy on clean data set for comaprision\n",
    "        test_acc_mv[i],mac=train_and_test(model_sgcn,data_bn,lam)\n",
    "        del model_sgcn\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        torch.manual_seed(i+2)\n",
    "        data_bn=create_noisy(datap,nl_veto)  \n",
    "        model_sgcn=SGCN(hidden_channels=16,seed=i+2)     # calculating accuracy on clean data set for comaprision\n",
    "        test_acc_veto[i],mac=train_and_test(model_sgcn,data_bn,lam)\n",
    "        del model_sgcn\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        torch.manual_seed(i+2)\n",
    "        data_bn=create_noisy(datap,nl_seq)  \n",
    "        model_sgcn=SGCN(hidden_channels=16,seed=i+2)     # calculating accuracy on clean data set for comaprision\n",
    "        test_acc_seq[i],mac=train_and_test(model_sgcn,data_bn,lam)\n",
    "        del model_sgcn\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"sln noise:\",np.mean(test_acc_sln),\"+-\",np.std(test_acc_sln))\n",
    "    print(\"majority vote noise:\",np.mean(test_acc_mv),\"+-\",np.std(test_acc_mv))\n",
    "    print(\"veto noise:\",np.mean(test_acc_veto),\"+-\",np.std(test_acc_veto))\n",
    "    print(\"sequential  noise:\",np.mean(test_acc_seq),\"+-\",np.std(test_acc_seq))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea997133-d16c-492a-9f3a-82a100eeb285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e492e582-651e-42c3-90b2-9b0d1f5de4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise level = 0.45\n",
      "ccn noise: 0.4895 +- 0.005500000000000005\n",
      "majority vote noise: 0.45399999999999996 +- 0.014999999999999986\n",
      "veto noise: 0.367 +- 0.016000000000000014\n",
      "Sequential Flipping: 0.4685 +- 0.048500000000000015\n"
     ]
    }
   ],
   "source": [
    "\n",
    "noise_levels=[0.45]\n",
    "iter=2\n",
    "for nl in noise_levels:\n",
    "    print(\"Noise level =\",nl)\n",
    "    pos=int(nl/0.05)-1\n",
    "    test_acc_sln=np.zeros(iter)\n",
    "\n",
    "    qn=q(rho_mv[pos],max_degree)\n",
    "    nl_majvote=torch.zeros(len(data.y))\n",
    "    for i in range(len(degree_list)):\n",
    "        nl_majvote[i]=float(qn[int(degree_list[i])-1])\n",
    "    nl_majvote.to(device)\n",
    "    test_acc_mv=np.zeros(iter)\n",
    "\n",
    "    qn=r(rho_veto[pos],max_degree)\n",
    "    nl_veto=torch.zeros(len(data.y))\n",
    "    for i in range(len(degree_list)):\n",
    "        nl_veto[i]=qn[int(degree_list[i])-1]\n",
    "    nl_veto.to(device)\n",
    "    test_acc_veto=np.zeros(iter)\n",
    "\n",
    "    qn=s_pw(rho_seq_pw[pos],max_degree)\n",
    "    nl_seq_pw=torch.zeros((len(data.y),no_of_classes))\n",
    "    for i in range(len(degree_list)):\n",
    "        nl_seq_pw[i]=torch.tensor(qn[int(degree_list[i])-1],dtype=torch.float64)\n",
    "    nl_seq_pw.to(device)\n",
    "    test_acc_seq=np.zeros(iter)\n",
    "    \n",
    "    for i in range(iter):\n",
    "        torch.manual_seed(i+2)\n",
    "        dataset = Planetoid(root='data/py311/Planetoid', name='Citeseer', split=\"random\", num_train_per_class=250,num_val=50,num_test=1000,transform=NormalizeFeatures())\n",
    "        datap = (dataset[0]).to(device)\n",
    "        data_bn=create_noisy_ccn(datap,nl)  \n",
    "        model_sgcn=SGCN(hidden_channels=16,seed=i+2)     # calculating accuracy on clean data set for comaprision\n",
    "        test_acc_sln[i],mac=train_and_test(model_sgcn,data_bn,lam)\n",
    "        del model_sgcn\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "        torch.manual_seed(i+2)\n",
    "        data_bn=create_noisy_ccn(datap,nl_majvote)  \n",
    "        model_sgcn=SGCN(hidden_channels=16,seed=i+2)     # calculating accuracy on clean data set for comaprision\n",
    "        test_acc_mv[i],mac=train_and_test(model_sgcn,data_bn,lam)\n",
    "        del model_sgcn\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        torch.manual_seed(i+2)\n",
    "        data_bn=create_noisy_ccn(datap,nl_veto)  \n",
    "        model_sgcn=SGCN(hidden_channels=16,seed=i+2)     # calculating accuracy on clean data set for comaprision\n",
    "        test_acc_veto[i],mac=train_and_test(model_sgcn,data_bn,lam)\n",
    "        del model_sgcn\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "        torch.manual_seed(i+2)\n",
    "        data_bn=create_noisy_seq_pw(datap,nl_seq_pw)  \n",
    "        model_sgcn=SGCN(hidden_channels=16,seed=i+2)     # calculating accuracy on clean data set for comaprision\n",
    "        test_acc_seq[i],mac=train_and_test(model_sgcn,data_bn,lam)\n",
    "        del model_sgcn\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"ccn noise:\",np.mean(test_acc_sln),\"+-\",np.std(test_acc_sln))\n",
    "    print(\"majority vote noise:\",np.mean(test_acc_mv),\"+-\",np.std(test_acc_mv))\n",
    "    print(\"veto noise:\",np.mean(test_acc_veto),\"+-\",np.std(test_acc_veto))\n",
    "    print(\"Sequential Flipping:\",np.mean(test_acc_seq),\"+-\",np.std(test_acc_seq))\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021e0a83-47a6-48cc-90cd-da712e8ed9e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69065c55-1951-4a2f-ba48-9c8ddaa5b1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
