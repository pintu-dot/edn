{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81e87209-e9c2-4f8a-8497-1d87ad738107",
   "metadata": {},
   "outputs": [],
   "source": [
    "lam=1e-4    # Regularisation parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f39b3d17-077d-4f05-a3e2-f2fcdb89b3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import networkx as nx\n",
    "from sklearn.metrics import f1_score\n",
    "import torch_geometric\n",
    "import random\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4637c5ac-4b03-4aec-97f9-44db7818d0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(model,data,lmbda):       # training and testing GCN models\n",
    "    data.to(device)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def train():\n",
    "        model.train()\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        reg = sum(p.pow(2.0).sum()\n",
    "                  for p in model.parameters())\n",
    "        out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])+lmbda*reg  # Compute the loss solely based on the training nodes.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        return loss\n",
    "\n",
    "    def test():\n",
    "        model.eval()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "        test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "        mac_f1=f1_score(data.y[data.test_mask].cpu(), pred[data.test_mask].cpu(), average='macro');\n",
    "        return test_acc,mac_f1\n",
    "\n",
    "\n",
    "    for epoch in range(1, 801):\n",
    "        loss = train()\n",
    "\n",
    "    test_acc,mac_f1 = test()\n",
    "    return test_acc,mac_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff234892-88d3-4052-adfc-585587bf3218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_val(model,data,lmbda):\n",
    "    data.to(device)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def train():\n",
    "        model.train()\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        reg = sum(p.pow(2.0).sum()\n",
    "                  for p in model.parameters())\n",
    "        out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])+lmbda*reg  # Compute the loss solely based on the training nodes.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        return loss\n",
    "\n",
    "    def val():\n",
    "        model.eval()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        val1=((True^data.train_mask)*(True^data.test_mask))\n",
    "        val_correct = pred[val1] == data.y[val1]  # Check against ground-truth labels.\n",
    "        val_acc = int(val_correct.sum()) / int(val1.sum())  # Derive ratio of correct predictions.\n",
    "        return val_acc\n",
    "    \n",
    "    def test():\n",
    "      model.eval()\n",
    "      out = model(data.x, data.edge_index)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "      mac_f1=f1_score(data.y[data.test_mask].cpu(), pred[data.test_mask].cpu(), average='macro');\n",
    "      return test_acc,mac_f1\n",
    "\n",
    "    for epoch in range(1, 801):\n",
    "        loss = train()\n",
    "\n",
    "    val_acc = val()\n",
    "    test_acc,mac_f1=test()\n",
    "\n",
    "    #print(f'Test Accuracy: {test_acc:.4f}')\n",
    "    return val_acc,test_acc,mac_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6a9e349-12c6-4671-9a98-52449459464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SGCN(torch.nn.Module):\n",
    "    def __init__(self,hidden_channels=16,seed=13):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.conv1 = GCNConv(dataset.num_features,10)\n",
    "        self.conv2=GCNConv(10,dataset.num_classes)\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x=F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5f39f90-456e-4450-a040-01f041e97e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_noisy(data,noise_level):   # Function to add SLN noise to data\n",
    "    data1=data.clone()\n",
    "\n",
    "    rand_num=torch.rand(len(data.y))\n",
    "    for k in range(no_of_classes):\n",
    "        for j in range(no_of_classes-1):\n",
    "            pos_to_flip=(((rand_num<(j+1)*noise_level/(no_of_classes-1)).to(device))*(((j)*noise_level/(no_of_classes-1)<=rand_num).to(device))*data.train_mask)*(data.y==k)\n",
    "            data1.y[pos_to_flip]=(data1.y[pos_to_flip]+(j+1))%no_of_classes\n",
    "    return(data1)\n",
    "\n",
    "def create_noisy_ccn(data,noise_level):\n",
    "    data1=data.clone()\n",
    "    rand_num=torch.rand(len(data.y))\n",
    "    pos_to_flip=(rand_num<=noise_level).to(device)*data1.train_mask\n",
    "    data1.y[pos_to_flip]=(data.y[pos_to_flip]+1)%no_of_classes\n",
    "    del data\n",
    "    torch.cuda.empty_cache()\n",
    "    return(data1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9df33656-29b0-4816-b703-9c4b36e5cf8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Planetoid(root='data/py311/Planetoid', name='Citeseer', split=\"random\", num_train_per_class=250,num_val=50,num_test=1000,transform=NormalizeFeatures())\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "datap=data.clone()\n",
    "no_of_classes=int((data.y.max()+1).detach())\n",
    "datap.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cbe97e7-25e2-4a06-ac4c-eb53a83bc423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def majvote(n,p):\n",
    "    val=np.float128(0);\n",
    "    p=np.float128(p)\n",
    "    for i in range(math.ceil(n/2),n+1):\n",
    "        val=val+np.float128(math.comb(n,i))*(p**i)*((1-p)**(n-i))\n",
    "    return np.float32(val)\n",
    "\n",
    "def q(p,max_degree):\n",
    "    q1=[]\n",
    "    for n in range(1,int(max_degree)+1):\n",
    "        q1.append(majvote(n,p))\n",
    "    return q1\n",
    "def veto_power(n,p):\n",
    "    return(1-((1-p)**n))\n",
    "\n",
    "def r(p,max_degree):\n",
    "    r1=[]\n",
    "    for n in range(1,int(max_degree)+1):\n",
    "        r1.append(veto_power(n,p))\n",
    "    return r1\n",
    "def sequential_mod(n,p):\n",
    "    k=no_of_classes\n",
    "    return (((k-1)/k)*(1-(1-(k*p)/(k-1))**(n)))\n",
    "\n",
    "def s_mod(p,max_degree):\n",
    "    r1=[]\n",
    "    for n in range(1,int(max_degree)+1):\n",
    "        r1.append(sequential_mod(n,p))\n",
    "    return r1\n",
    "def s(p,max_degree):  # for sequential flipping\n",
    "    s1=[p]\n",
    "    temp=p;\n",
    "    for n in range(2,int(max_degree)+1):\n",
    "        temp=p+temp*(1-2*p)\n",
    "        s1.append(temp)\n",
    "    return s1\n",
    "\n",
    "def rho_calculate(data,type='majority_vote',):\n",
    "    g_nx=torch_geometric.utils.convert.to_networkx(data,to_undirected='True',remove_self_loops='True')\n",
    "    max_degree=np.max(np.array(g_nx.degree)[:,1])/2\n",
    "    unique, counts = np.unique(np.array(g_nx.degree)[:,1], return_counts=True)\n",
    "    unique=unique/2\n",
    "    no_nodes=counts.sum()\n",
    "    x = np.linspace(0,1,5000)\n",
    "    y = expected_noise(x,unique,counts,max_degree,no_nodes,type)\n",
    "    noise_lev=[0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5]\n",
    "    x_intersect=np.zeros(len(noise_lev))\n",
    "    for j,i in enumerate(noise_lev):\n",
    "        x_intersect[j]=(x[np.argmin(np.abs(y - i))])\n",
    "    return x_intersect\n",
    "def expected_noise(p,unique,counts,max_degree,no_nodes,type='majority_vote'):\n",
    "    if type=='majority_vote':\n",
    "        qn=q(p,max_degree)\n",
    "    elif type=='veto_power':\n",
    "        qn=r(p,max_degree)\n",
    "    elif type=='sequential':\n",
    "        qn=s_mod(p,max_degree)\n",
    "    deg_dist=np.zeros(int(max_degree))\n",
    "    for i,j in enumerate(unique):\n",
    "        deg_dist[int(j)-1]=counts[i]/no_nodes\n",
    "    exp_noise=np.dot(deg_dist,np.array(qn))\n",
    "    return exp_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6c84f71-149b-4e88-b8d2-20d6eed2780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "g_nx=torch_geometric.utils.convert.to_networkx(data,to_undirected='True',remove_self_loops='True')\n",
    "max_degree=np.max(np.array(g_nx.degree)[:,1])/2\n",
    "degree_list=(np.array(g_nx.degree)[:,1])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa5eeb99-2af6-4b22-bba2-e7bbced98690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimating rho_values\n",
    "rho_mv=rho_calculate(data,'majority_vote')\n",
    "rho_veto=rho_calculate(data, 'veto_power')\n",
    "rho_seq=rho_calculate(data, 'sequential')\n",
    "                       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2aa9e5ce-de68-421d-8b6a-2635a3bbc0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_nx=torch_geometric.utils.convert.to_networkx(data,to_undirected='True',remove_self_loops='True')\n",
    "max_degree=np.max(np.array(g_nx.degree)[:,1])/2\n",
    "unique, counts = np.unique(np.array(g_nx.degree)[:,1], return_counts=True)\n",
    "unique=unique/2\n",
    "no_nodes=counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d9da14-9095-4e36-9045-de0d53a924b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "978102bb-4a14-476c-809d-06c7e150c286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise level = 0.05\n",
      "sln noise: 0.7536 +- 0.012603174203350533\n",
      "majority vote noise: 0.7526 +- 0.005868560300448489\n",
      "veto noise: 0.7536 +- 0.005851495535331123\n",
      "sequential  noise: 0.7522 +- 0.006720119046564584\n",
      "Noise level = 0.1\n",
      "sln noise: 0.7381 +- 0.012770669520428452\n",
      "majority vote noise: 0.7412 +- 0.006867313885355764\n",
      "veto noise: 0.7327999999999999 +- 0.003789459064299287\n",
      "sequential  noise: 0.7341 +- 0.004205948168962623\n",
      "Noise level = 0.15\n",
      "sln noise: 0.7264 +- 0.013062924634246357\n",
      "majority vote noise: 0.7414000000000001 +- 0.0068000000000000066\n",
      "veto noise: 0.7336 +- 0.004800000000000005\n",
      "sequential  noise: 0.7334 +- 0.004247352116319064\n",
      "Noise level = 0.2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m data_bn\u001b[38;5;241m=\u001b[39mcreate_noisy(datap,nl)  \n\u001b[1;32m     12\u001b[0m model_sgcn\u001b[38;5;241m=\u001b[39mSGCN(hidden_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,seed\u001b[38;5;241m=\u001b[39mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m)     \u001b[38;5;66;03m# calculating accuracy on clean data set for comaprision\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m test_acc_sln[i],mac\u001b[38;5;241m=\u001b[39mtrain_and_test(model_sgcn,data_bn,lam)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model_sgcn\n\u001b[1;32m     15\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[0;32mIn[2], line 29\u001b[0m, in \u001b[0;36mtrain_and_test\u001b[0;34m(model, data, lmbda)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m test_acc,mac_f1\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m801\u001b[39m):\n\u001b[0;32m---> 29\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train()\n\u001b[1;32m     31\u001b[0m test_acc,mac_f1 \u001b[38;5;241m=\u001b[39m test()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m test_acc,mac_f1\n",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m, in \u001b[0;36mtrain_and_test.<locals>.train\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Clear gradients.\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m reg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2.0\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     11\u001b[0m           \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m     12\u001b[0m out \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index)  \u001b[38;5;66;03m# Perform a single forward pass.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out[data\u001b[38;5;241m.\u001b[39mtrain_mask], data\u001b[38;5;241m.\u001b[39my[data\u001b[38;5;241m.\u001b[39mtrain_mask])\u001b[38;5;241m+\u001b[39mlmbda\u001b[38;5;241m*\u001b[39mreg  \u001b[38;5;66;03m# Compute the loss solely based on the training nodes.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Clear gradients.\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m reg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2.0\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     11\u001b[0m           \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m     12\u001b[0m out \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index)  \u001b[38;5;66;03m# Perform a single forward pass.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out[data\u001b[38;5;241m.\u001b[39mtrain_mask], data\u001b[38;5;241m.\u001b[39my[data\u001b[38;5;241m.\u001b[39mtrain_mask])\u001b[38;5;241m+\u001b[39mlmbda\u001b[38;5;241m*\u001b[39mreg  \u001b[38;5;66;03m# Compute the loss solely based on the training nodes.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "noise_levels=[0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5]\n",
    "iter=10\n",
    "for nl in noise_levels:\n",
    "    print(\"Noise level =\",nl)\n",
    "    pos=int(nl/0.05)-1\n",
    "    test_acc_sln=np.zeros(iter)\n",
    "    for i in range(iter):\n",
    "        torch.manual_seed(i+2)\n",
    "        dataset = Planetoid(root='data/py311/Planetoid', name='Citeseer', split=\"random\", num_train_per_class=250,num_val=50,num_test=1000,transform=NormalizeFeatures())\n",
    "        datap = (dataset[0]).to(device)\n",
    "        data_bn=create_noisy(datap,nl)  \n",
    "        model_sgcn=SGCN(hidden_channels=16,seed=i+2)     # calculating accuracy on clean data set for comaprision\n",
    "        test_acc_sln[i],mac=train_and_test(model_sgcn,data_bn,lam)\n",
    "        del model_sgcn\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"sln noise:\",np.mean(test_acc_sln),\"+-\",np.std(test_acc_sln))\n",
    "    \n",
    "    qn=q(rho_mv[pos],max_degree)\n",
    "    nl_majvote=torch.zeros(len(data.y))\n",
    "    for i in range(len(degree_list)):\n",
    "        nl_majvote[i]=float(qn[int(degree_list[i])-1])\n",
    "    nl_majvote.to(device)\n",
    "    test_acc_mv=np.zeros(iter)\n",
    "    for i in range(iter):\n",
    "        torch.manual_seed(i+2)\n",
    "        data_bn=create_noisy(datap,nl_majvote)  \n",
    "        model_sgcn=SGCN(hidden_channels=16,seed=i+2)     # calculating accuracy on clean data set for comaprision\n",
    "        test_acc_mv[i],mac=train_and_test(model_sgcn,data_bn,lam)\n",
    "        del model_sgcn\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"majority vote noise:\",np.mean(test_acc_mv),\"+-\",np.std(test_acc_mv))\n",
    "\n",
    "    qn=r(rho_veto[pos],max_degree)\n",
    "    nl_veto=torch.zeros(len(data.y))\n",
    "    for i in range(len(degree_list)):\n",
    "        nl_veto[i]=qn[int(degree_list[i])-1]\n",
    "    nl_veto.to(device)\n",
    "    test_acc_veto=np.zeros(iter)\n",
    "    for i in range(iter):\n",
    "        torch.manual_seed(i+2)\n",
    "        data_bn=create_noisy(datap,nl_veto)  \n",
    "        model_sgcn=SGCN(hidden_channels=16,seed=i+2)     # calculating accuracy on clean data set for comaprision\n",
    "        test_acc_veto[i],mac=train_and_test(model_sgcn,data_bn,lam)\n",
    "        del model_sgcn\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"veto noise:\",np.mean(test_acc_veto),\"+-\",np.std(test_acc_veto))\n",
    "\n",
    "    qn=s_mod(rho_seq[pos],max_degree)\n",
    "    nl_seq=torch.zeros(len(data.y))\n",
    "    for i in range(len(degree_list)):\n",
    "        nl_seq[i]=qn[int(degree_list[i])-1]\n",
    "    nl_seq.to(device)\n",
    "    test_acc_seq=np.zeros(iter)\n",
    "    for i in range(iter):\n",
    "        torch.manual_seed(i+2)\n",
    "        data_bn=create_noisy(datap,nl_seq)  \n",
    "        model_sgcn=SGCN(hidden_channels=16,seed=i+2)     # calculating accuracy on clean data set for comaprision\n",
    "        test_acc_seq[i],mac=train_and_test(model_sgcn,data_bn,lam)\n",
    "        del model_sgcn\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"sequential  noise:\",np.mean(test_acc_seq),\"+-\",np.std(test_acc_seq))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea997133-d16c-492a-9f3a-82a100eeb285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e492e582-651e-42c3-90b2-9b0d1f5de4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise level = 0.05\n",
      "ccn noise: 0.7544000000000001 +- 0.011867602959317448\n",
      "majority vote noise: 0.7525999999999999 +- 0.006406246951218792\n",
      "veto noise: 0.7515000000000001 +- 0.005869412236331681\n",
      "Noise level = 0.1\n",
      "ccn noise: 0.7442999999999999 +- 0.010109896141899785\n",
      "majority vote noise: 0.7407999999999999 +- 0.00804735981549229\n",
      "veto noise: 0.7304 +- 0.008879189152169254\n",
      "Noise level = 0.15\n",
      "ccn noise: 0.7262000000000001 +- 0.014647866738880456\n",
      "majority vote noise: 0.7405999999999999 +- 0.008151073548925941\n",
      "veto noise: 0.7307 +- 0.008989438247187648\n",
      "Noise level = 0.2\n",
      "ccn noise: 0.7043 +- 0.015199013125857872\n",
      "majority vote noise: 0.7044999999999999 +- 0.006545991139621263\n",
      "veto noise: 0.6658999999999999 +- 0.014679577650600163\n",
      "Noise level = 0.25\n",
      "ccn noise: 0.6761000000000001 +- 0.016176835290006485\n",
      "majority vote noise: 0.6753 +- 0.016377118183612142\n",
      "veto noise: 0.6178 +- 0.01893568060567142\n",
      "Noise level = 0.3\n",
      "ccn noise: 0.6302000000000001 +- 0.01643045951883271\n",
      "majority vote noise: 0.6756 +- 0.01608850521335029\n",
      "veto noise: 0.6174 +- 0.018232937229091768\n",
      "Noise level = 0.35\n",
      "ccn noise: 0.5901 +- 0.022677962871475006\n",
      "majority vote noise: 0.6416 +- 0.015602563891873684\n",
      "veto noise: 0.5714 +- 0.0277027074489119\n",
      "Noise level = 0.4\n",
      "ccn noise: 0.5385 +- 0.024808264751892652\n"
     ]
    }
   ],
   "source": [
    "\n",
    "noise_levels=[0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5]\n",
    "iter=10\n",
    "for nl in noise_levels:\n",
    "    print(\"Noise level =\",nl)\n",
    "    pos=int(nl/0.05)-1\n",
    "    test_acc_sln=np.zeros(iter)\n",
    "    for i in range(iter):\n",
    "        torch.manual_seed(i+2)\n",
    "        dataset = Planetoid(root='data/py311/Planetoid', name='Citeseer', split=\"random\", num_train_per_class=250,num_val=50,num_test=1000,transform=NormalizeFeatures())\n",
    "        datap = (dataset[0]).to(device)\n",
    "        data_bn=create_noisy_ccn(datap,nl)  \n",
    "        model_sgcn=SGCN(hidden_channels=16,seed=i+2)     # calculating accuracy on clean data set for comaprision\n",
    "        test_acc_sln[i],mac=train_and_test(model_sgcn,data_bn,lam)\n",
    "        del model_sgcn\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"ccn noise:\",np.mean(test_acc_sln),\"+-\",np.std(test_acc_sln))\n",
    "    \n",
    "    qn=q(rho_mv[pos],max_degree)\n",
    "    nl_majvote=torch.zeros(len(data.y))\n",
    "    for i in range(len(degree_list)):\n",
    "        nl_majvote[i]=float(qn[int(degree_list[i])-1])\n",
    "    nl_majvote.to(device)\n",
    "    test_acc_mv=np.zeros(iter)\n",
    "    for i in range(iter):\n",
    "        torch.manual_seed(i+2)\n",
    "        data_bn=create_noisy_ccn(datap,nl_majvote)  \n",
    "        model_sgcn=SGCN(hidden_channels=16,seed=i+2)     # calculating accuracy on clean data set for comaprision\n",
    "        test_acc_mv[i],mac=train_and_test(model_sgcn,data_bn,lam)\n",
    "        del model_sgcn\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"majority vote noise:\",np.mean(test_acc_mv),\"+-\",np.std(test_acc_mv))\n",
    "\n",
    "    qn=r(rho_veto[pos],max_degree)\n",
    "    nl_veto=torch.zeros(len(data.y))\n",
    "    for i in range(len(degree_list)):\n",
    "        nl_veto[i]=qn[int(degree_list[i])-1]\n",
    "    nl_veto.to(device)\n",
    "    test_acc_veto=np.zeros(iter)\n",
    "    for i in range(iter):\n",
    "        torch.manual_seed(i+2)\n",
    "        data_bn=create_noisy_ccn(datap,nl_veto)  \n",
    "        model_sgcn=SGCN(hidden_channels=16,seed=i+2)     # calculating accuracy on clean data set for comaprision\n",
    "        test_acc_veto[i],mac=train_and_test(model_sgcn,data_bn,lam)\n",
    "        del model_sgcn\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"veto noise:\",np.mean(test_acc_veto),\"+-\",np.std(test_acc_veto))\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021e0a83-47a6-48cc-90cd-da712e8ed9e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
